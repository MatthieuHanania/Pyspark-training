{"cells":[{"cell_type":"markdown","source":["# Natural Language Processing (NLP)\n\n- NLP is a very large field of machine learning that focuses on creating models from a text data source \n- This course is a rapid presentation of tools provided by Spark machine learning library that facilitate constructing these models.  \n- Applications of NLP are numerous:\n\n    - Books recommendations\n    - News articles clustering\n    - Sentimental analysis\n    - Spam detection\n    - Text generation\n    - Language translation\n    - chatbots\n    - etc. \n\n## Terminology\n\n### Document\n- A document can be an email, a text feedback, an sms, an article, a book, etc.. It is represented as a vector of word counts is called a â€œBag of Words\n\n### Corpus\n- corpus (plural, \"corpora\") is a set of documents\n\n## Process\n- Compile the corpus and create a DataFrame\n- Featurization: it is about preparing the features in order to create the model. This may includes:\n\n    - Tokenization. Spark tools in pyspark.ml.feature: Tokenizer, RegexTokenizer\n    - Stop Words Removing. Spark tools in pyspark.ml.feature: StopWordsRemover\n    - Using NGrams. Spark tools in pyspark.ml.feature: NGram\n    - Numeric Features. Spark tools in pyspark.ml.feature include HashingTF, IDF, CountVectorizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7cb9cc52-b0fa-43f5-84b0-039eee4d7487","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df= spark.createDataFrame([\n    (0, 'Hi Spark is good'),\n    (1, 'Java is good choice for beginners'),\n    (0, 'In,Spark,you can, do, logistic,Regression;NLP'),\n    (0, 'Thanks for using spark. Many corporates will appreciate that!'),\n    (1, 'Java is  fine'),\n    (1, 'Learn about re on  https://docs.python.org/3/library/re.html'),\n    (1, 'This is a quick itroduction to spark ML'),\n    (1, 'Java is not the preferred language of Data Scientists') \n], ['label', 'text']\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c18a66cc-298e-4c95-a213-cd7189527b8c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0e6c25f-6c29-4399-84bd-74335f58aea7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e8a4056-9a88-490d-bbc1-e51654ab383b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Start with Tokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf09fa4a-2c75-48eb-a5e3-1eb75d3beebf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer\ntokenizer = Tokenizer(inputCol='text', outputCol='words')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e7bf9c8-04c7-4f80-9352-7d12f0a487ad","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokenized_df = tokenizer.transform(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11f8333f-df53-4b4c-840d-4e979df7eccc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(tokenized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cdf77cda-bb6b-4043-935a-c53a7f301c48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokenized_output = tokenized_df.collect()\nfor document in tokenized_output:\n  for token in document:\n    print (token)\n "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"193535d0-d9fd-4130-8f1d-85ddf98cd500","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokenized_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46d3f6e0-45ab-4bb1-aabc-649bd8250553","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokenized_df.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b34c510c-1504-4c97-90f4-250ba221c642","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import udf \nfrom pyspark.sql.types import IntegerType\ncount_words = udf(lambda words: len(words), IntegerType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f00fecfc-69f1-4821-94eb-5fc15735c4a0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokenized_df.withColumn('counts', count_words('words')).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f06df79f-3da7-4570-93ae-2fce6b1e5a0c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Now let's use RegexTokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef4be9f3-67d3-46eb-b302-b9e875e22a41","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["regex_tokenizer = RegexTokenizer(inputCol='text', outputCol='words', pattern='\\\\W')\n#regex_tokenizer.setMinTokenLength(4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ed163e8-8127-4905-9454-71c24af7e9b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## More on regular expressions with Python\nMore on:\nhttps://docs.python.org/3/library/re.html\n\n### \\W\nMatches any character which is not a word character. This is the opposite of \\w. If the ASCII flag is used this becomes the equivalent of [^a-zA-Z0-9_] (but the flag affects the entire regular expression, so in such cases using an explicit [^a-zA-Z0-9_] may be a better choice)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9cc44144-35d8-4bb7-b3a6-095f55b2038c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["regex_df = regex_tokenizer.transform(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ff57e69-4e9d-431d-a96a-69269a3ffa1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["regex_tokenized_counts = regex_df.withColumn('len', count_words('words'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c69bc4cb-8933-442d-90f3-27a6cb13fb8b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["regex_tokenized_counts.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b10857f9-7d3f-4472-b326-9270cc2a3832","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for item in regex_tokenized_counts.collect()[5]:\n    print (item)\n    \n    \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f33539e-83e3-4635-8ee3-9fee183042a6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(regex_tokenized_counts)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6da7a549-8a9f-4cd4-b8cf-e41df9993236","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# With RegexTokenizer, we can remove small words"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de3e3cb9-efce-4884-8b6c-9f44e84e3b14","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["regex_tokenizer_min_4 = RegexTokenizer(inputCol='text', outputCol='words', pattern='\\\\W')\nregex_tokenizer_min_4.setMinTokenLength(4)\nregex_min_4_df = regex_tokenizer_min_4.transform(df)\nregex_tokenized_min_4_counts = regex_min_4_df.withColumn('count', count_words('words'))\nregex_tokenized_min_4_counts.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"470111b6-105e-4cab-9fdf-fb9b6475bc7f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n for item in regex_tokenized_min_4_counts.collect()[5]:\n    print (item)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fa4ab52-b3f3-4b4f-b5a0-54373bf71c8f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Once we have the tokens, we can remove _stop-words_ by using _StopWordsRemover_"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b003724-d980-4726-a0b1-996e1ea27ab7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c30d323-1eda-42d0-905a-27c51b638b54","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["remover = StopWordsRemover(inputCol='words', outputCol='tokens')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02ec89bd-722e-4df3-932b-ba5898879b1f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokens_filtered = remover.transform(regex_tokenized_counts)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"104e86be-485a-4ae3-ac23-294733432156","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cleanDF= tokens_filtered.withColumn('count_tokens', count_words('tokens'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a75bc544-2752-440e-8991-15a58cd47f3b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display (cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23c9216d-faaa-4fde-a8e1-00e5434eef9d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cleanDF.select('words', 'len', 'tokens', 'count_tokens').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee60b070-e171-4142-97bd-ef61fd3d22fb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cleanDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a703689e-b68c-4c8c-9e96-7c95e3ff5c0f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for item in cleanDF.collect()[5]:\n   print(item)    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5508825e-9c06-46f7-8f0a-19f838c10c80","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Defining a custom remove words list"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1823deee-7ea1-4c50-8018-02ebd793e7f3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["remover.getStopWords()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afbfec37-3a06-44b0-bf79-3defcad38f51","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stopWords=['a', 'is', 'for', 'hi', 'in', 'on']\nremover.setStopWords(stopWords)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74259fd6-dbaa-4deb-9a17-9b3588a46fd3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["remover.transform(regex_tokenized_counts).select('text', 'tokens').show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"243117ba-3c18-4091-a3e1-1426a3470862","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["newCleanDF=remover.transform(regex_tokenized_counts).withColumn('count_tokens', count_words('tokens'))\nnewCleanDF.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2abe2615-89cc-48bf-927f-6c970c4495ee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for item in newCleanDF.collect()[4]:\n   print(item)  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f574ac01-7c82-49e1-be82-bc5e43da5b65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# NGRAM\nA feature transformer that converts the input array of strings into an array of n-grams"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4880ada-f280-4588-82cd-edd1f52ebc51","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import NGram"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f393dbee-2292-4756-9225-64e735bebacf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8ced7c8-ff90-4b07-a878-fa3f4eed20f4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cleanDF.collect()[3]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21071fc2-d231-4c14-87f3-872717ffeb65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ngram = NGram(n=2, inputCol='tokens', outputCol='2grams')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1e3e273-79b7-4b3e-90dd-9eb7aa63c108","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["my_2ngrams =ngram.transform(cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"349c133a-7564-4240-be81-ba22e2793d66","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display (my_2ngrams)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d108c5d6-57e5-4709-be4b-5d563861fd4e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["my_2ngrams.select('2grams').show(truncate =False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a5d4c6b-9f97-4b88-a073-9c0f8ee143ef","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for item in my_2ngrams.collect()[6]:\n    print (item)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"867cb0b8-fce9-4935-b0f1-1da7c7903523","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# TF: Term Frequency\nMaps a sequence of terms to their term frequencies using the hashing trick.\n\nNote: the terms must be hashable (can not be dictionary or list...).\n\nHashingTF(S) takes the hash code of each word modulo the desired vector size S, and thus maps each word to a number between 0 and S-1.\n\nThis yields a quite robust vector even if multiple words may map to the same hash code. \n\nSpark Machine Learning developers recommend setting S between $$ 2^{18}    \\&    2^{20} $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"693f83ba-6995-4813-97ce-4101203d5d6f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Vector Size and tradeoff:\n- S = 1000 \n- TF (mot1) = Hash(mot1) % 1000 = 2002 % 1000 = 2\n- TF (mot2) = Hash(mot2) % 1000 = 120013 % 1000 = 13\n- ...\n- ....\n- TF (mot1000) = Hash(mot1000) % 1000 = 122002 % 1000 = 2\n- TF (mot1) = TF (mot1000) (before frequency)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d68c241b-b971-4705-9e4d-a8f51f514612","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01d2b687-0d1a-4e22-bce0-6a92ac69e700","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cleanDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78158952-7dba-499f-bf62-e07a901c014c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tf = HashingTF(numFeatures=1000, inputCol='tokens', outputCol='features')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59f197e5-c5c1-48e4-886f-5a27b43256fb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tf.explainParams()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"099f8000-b30e-401d-a294-ce5f0cf91417","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tf.explainParam('numFeatures')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"052a058d-cf18-4eca-b6c4-ed9f75642178","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tf.setNumFeatures(200056)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de8eebf1-1aad-48e1-9941-e1dd44e0f4d8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eab636b7-67be-48a9-8586-8dc9de9522c7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print (tf.getNumFeatures())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2be417db-b9a0-4f10-bc97-7770ce4320f0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tf_df = tf.transform(cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c120dd0-5442-45ba-a2d6-4bf732df9808","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(tf_df)"],"metadata":{"scrolled":true,"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f983e538-56aa-4db6-8585-d3e1950240d9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for item in tf_df.collect()[3]:\n    print (item)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"717d659c-f211-4d0e-8b02-70e0b59ff440","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Â TF-IDF: Term Frequence - Inverse Document Frequency\n\nOnce you have TF vectors, you can use IDF to compute the inverse document frequencies and multiply them with the TF to compute the TF-IDF \n\nIDF measures how infrequently a term occurs across the whole document corpus\n\nTF x IDF shows how relevant a term is to specific document (i.e., if it is common in that document but rare in the whole corpus)\n\nTF-IDF is used to improve on Bag of Words by adjusting word counts based on their frequency in the corpus\n\n## How to calculate them?\nVarious ways for determining the exact values of both statistics exist:\n\n- TF(x, y): number of occurences of term x in document y. It represents the importance of a term in the document. \n\n- IDF(t): Importance of the term in the document. \n\n$$IDF(t)= log\\frac{N}{N(t)}$$\n- N: number of documents in the corpus D, N=|D|\n- N(t): Number of the documents where the term t appears (i.e: TF(t, d)!=0). N(t)= |{d in D, t in D}|\n- TF-IDF(t, d) =TF(t, d)  \n\n## Further Information on TF-IDF\nhttps://fr.wikipedia.org/wiki/TF-IDF\n\n## Preparing Data to TF-IDF\nIn a real pipeline, you will likely need to preprocess and stem words before passing them to TF.\n\nEx: convert words to lowercase, drop punctuation characters or drop suffixes like â€˜ingâ€™.\n\nYou can use external single node natural language libraries like NLTK (http://www.nltk.org)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a5f9c70-ddd2-4a4d-a788-a29023bfd7fe","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["idf = IDF(inputCol='features', outputCol='idf_features')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a99d221-ac2e-4cf5-8a30-cd541e27ff72","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["idf_model = idf.fit(tf_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5f4c8f0-5a5c-400d-8b73-5399462c579e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data=idf_model.transform(tf_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1da9e5c-0bac-428c-b796-bdf9f9352272","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c31ef7ad-a012-43fa-aa5c-4da7122caf82","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7802887b-3c33-45df-b256-6627b639f214","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for term in data.collect()[4]:\n    print (term)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ada84031-4960-4c37-bfa0-b844b8763592","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data.select('tokens').show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"220c63ce-1ad7-4c42-9390-aa07a83f07c5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"879ecf22-b54f-467b-9de0-38a7b78f02e2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ll = LogisticRegression(featuresCol='idf_features', labelCol='label')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2351054-13de-456b-8aa2-bf4395054689","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train, test = data.randomSplit([0.7, 0.3])\ntrain.cache()\ntest.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d88023c-a94c-4b07-8a9c-792c54d77af0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(metricName='f1', predictionCol='prediction') #  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1aad3cf9-7758-4bbe-b437-aeedab4c9f58","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model = ll.fit(train)\nresults = model.transform(test)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"553578ef-169d-4b08-ad26-f72b816683da","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print('evaluation of logistic regression model = %g'%evaluator.evaluate(results))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1de58112-d1a7-4406-8e1e-4c87a78020ec","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Vectorizing with CountVectorizer (another option)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a0b2031-7d4d-4d8c-9a4e-241dc2180085","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"42607ef1-658e-4619-9cc7-d4465ad230d6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20ddc638-8403-4267-84e9-f9eb855e5656","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec = CountVectorizer(inputCol='tokens', outputCol='features',  minDF=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"472cf64c-29a4-482b-836c-bde7624ce700","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec.explainParam('maxDF')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ed6a336-bb9c-4a0a-be1d-b5da6dea33cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model = count_vec.fit(cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d72c2164-d33c-4ac1-9e49-9335cb9fa296","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = model.transform(cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f46d58db-9cbe-4079-a431-c37242c787d2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data.select(['tokens', 'features']).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d68baba7-4176-493a-a55f-a259dd4de4a0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec = CountVectorizer(inputCol='tokens', outputCol='features', minDF=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68f48103-56ce-46b9-91a0-c5e9d1f26435","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["help(CountVectorizer)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2590cf4d-869f-4574-a46a-3028809a6837","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec.explainParam('minDF')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db61be48-51a9-4707-a928-d5cf0d859f67","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec_df=count_vec.fit(cleanDF).transform(cleanDF).select('tokens', 'features')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d224dce8-b54c-42cc-a43b-fa9bd99e0529","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec_df.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"201e78a0-17c7-42df-ab4c-5fa82412112d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10e0f384-2505-4282-b3fa-d0849c4862aa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec = CountVectorizer(inputCol='tokens', outputCol='features', vocabSize=15, minDF=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c916120-32da-4cc9-ae02-f9c2d954961d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vec.fit(cleanDF).transform(cleanDF).select('tokens', 'features').show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7545866-bd4b-4be9-97b8-8d739d09f066","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.classification import LogisticRegression, NaiveBayes, MultilayerPerceptronClassifier, RandomForestClassifier"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5794503e-5d11-4282-8d79-41e2eefe6efc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3a722ad-081e-45a6-a0f8-188c3d880dad","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"NLP_StartWith (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
